{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":73291,"databundleVersionId":8930475,"sourceType":"competition"},{"sourceId":7349720,"sourceType":"datasetVersion","datasetId":4268036}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Insurance Cross-Selling: Binary Classification Prediction| PS4E7","metadata":{}},{"cell_type":"markdown","source":"# Data Details:\n\n* Rows: 11,504,798\n* Columns: 12\n* This is the Binary Classification of Insurance Cross Selling Dataset.\n* It consists of three Files such as Train, test and sample submission\n* The Training data consists of 11504798 rows and 12 columns\n* The columns of Training Data are id, Gender, Age, Driving_License, Region_Code, Previously_Insured, Vehicle_Age, Vehicle_Damage, Annual_Premium, Policy_Sales_Channel, Vintage, Response.\n* The test Data consists of 7669866 rows and 11 columns.\n* The test Data includes these columns such as id, Gender, Age, Driving_License, Region_Code, Previously_Insured, Vehicle_Age, Vehicle_Damage, Annual_Premium, Policy_Sales_Channel, Vintage.\n* while the sample submission consists of 7669866 rows and 2 columns.\n* The columns of sample submission includes id, Response.\n* The dataset for this competition (both train and test) was generated from a deep learning model trained on the Health Insurance Cross Sell Prediction Data dataset","metadata":{}},{"cell_type":"markdown","source":"\n# Files:\n* train.csv - the training dataset; Response is the binary target\n* test.csv - the test dataset; your objective is to predict the probability of Response for each row\n* sample_submission.csv - a sample submission file in the correct format","metadata":{}},{"cell_type":"markdown","source":"# Objectives\n\n\n* This is Binary Classification of Insurance Cross Selling Competition Data.\n* The aim to take this is to predict which customers respond positively to an automobile insurance offer.\n* For this I firstly took the detailed overview about the data and then I create various visualization plots in the form of subplots such as countplot, pairplot, violin plot and piechart to take deep insights about Data.\n* Then I visualize the outliers tthrough Boxplot and apply the winsorization to take accuracte and best results.\n* Then I apply the enhanced Feature Engineering.\n* Then do the model Training and Stacking to take the enhanced predictions","metadata":{}},{"cell_type":"markdown","source":"# About Training Data\n\n* Rows: 11,504,798\n* Columns: 12\n* id: Unique identifier for each record\n* Gender: Gender of the individual\n* Age: Age of the individual\n* Driving_License: Indicates if the individual has a driving license\n* Region_Code: Code representing the region\n* Previously_Insured: Indicates if the individual was previously insured\n* Vehicle_Age: Age of the vehicle\n* Vehicle_Damage: Indicates if the vehicle has damage\n* Annual_Premium: Annual premium amount\n* Policy_Sales_Channel: Channel through which the policy was sold\n* Vintage: Number of days since the individual became a customer\n* Response: Target variable indicating whether the individual will buy the insurance (binary)","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-08-10T05:26:09.188153Z","iopub.execute_input":"2024-08-10T05:26:09.188504Z","iopub.status.idle":"2024-08-10T05:26:11.023607Z","shell.execute_reply.started":"2024-08-10T05:26:09.188475Z","shell.execute_reply":"2024-08-10T05:26:11.022626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install lightgbm","metadata":{"execution":{"iopub.status.busy":"2024-08-10T05:26:11.024967Z","iopub.execute_input":"2024-08-10T05:26:11.025379Z","iopub.status.idle":"2024-08-10T05:26:15.576886Z","shell.execute_reply.started":"2024-08-10T05:26:11.025333Z","shell.execute_reply":"2024-08-10T05:26:15.575757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import Libraries\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nfrom IPython.display import display, HTML\nimport matplotlib.patches as patches\nfrom scipy.stats.mstats import winsorize\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-08-10T05:26:15.578277Z","iopub.execute_input":"2024-08-10T05:26:15.578595Z","iopub.status.idle":"2024-08-10T05:26:18.646913Z","shell.execute_reply.started":"2024-08-10T05:26:15.578563Z","shell.execute_reply":"2024-08-10T05:26:18.646180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Understanding","metadata":{}},{"cell_type":"code","source":"# Function to style tables\ndef style_table(df):\n    styled_df = df.style.set_table_styles([\n        {\"selector\": \"th\", \"props\": [(\"color\", \"white\"), (\"background-color\", \"#6a1b9a\")]}\n    ]).set_properties(**{\"text-align\": \"center\"}).hide(axis=\"index\")\n    return styled_df.to_html()\n\n# Function to generate random shades of color\ndef generate_random_color():\n    color = \"#{:02x}{:02x}{:02x}\".format(\n        random.randint(150, 255),\n        random.randint(150, 255),\n        random.randint(150, 255)\n    )\n    return color\n\n# Function to create styled heading with emojis\ndef styled_heading(text, background_color='#6a1b9a', text_color='white'):\n    return f\"\"\"\n    <div style=\"\n        text-align: center;\n        background: {background_color};\n        color: {text_color};\n        padding: 20px;\n        font-size: 24px;\n        font-weight: bold;\n        border-radius: 10px;\n        margin-bottom: 20px;\n        box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);\n        border: 2px solid {background_color};\n    \">\n        {text}\n    </div>\n    \"\"\"\n\ndef print_dataset_analysis(dataset, dataset_name, n_top=5, heading_color='#6a1b9a', text_color='white'):\n    heading = styled_heading(f\"üìä {dataset_name} Overview\", heading_color, text_color)\n    display(HTML(heading))\n    \n    display(HTML(\"<h2 style='font-size: 20px; margin-top: 20px;'>üìè Shape of the Dataset</h2>\"))\n    display(HTML(f\"<p>{dataset.shape[0]} rows and {dataset.shape[1]} columns</p>\"))\n    \n    display(HTML(\"<h2 style='font-size: 20px; margin-top: 20px;'>üîç First 5 Rows</h2>\"))\n    display(HTML(style_table(dataset.head(n_top))))\n    \n    display(HTML(\"<h2 style='font-size: 20px; margin-top: 20px;'>üìä Summary Statistics</h2>\"))\n    display(HTML(style_table(dataset.describe())))\n    \n    display(HTML(\"<h2 style='font-size: 20px; margin-top: 20px;'>üîß Null Values</h2>\"))\n    null_counts = dataset.isnull().sum()\n    if null_counts.sum() == 0:\n        display(HTML(\"<p>No null values found.</p>\"))\n    else:\n        display(HTML(style_table(null_counts[null_counts > 0].to_frame(name='Null Values'))))\n    \n    display(HTML(\"<h2 style='font-size: 20px; margin-top: 20px;'>‚ôªÔ∏è Duplicate Rows</h2>\"))\n    duplicate_count = dataset.duplicated().sum()\n    display(HTML(f\"<p>{duplicate_count} duplicate rows found.</p>\"))\n    \n    display(HTML(\"<h2 style='font-size: 20px; margin-top: 20px;'>üóÇÔ∏è Data Types</h2>\"))\n    dtypes_table = pd.DataFrame({\n        'Column Name': dataset.columns,\n        'Data Type': [dataset[col].dtype for col in dataset.columns]\n    })\n    display(HTML(style_table(dtypes_table)))\n    \n    display(HTML(\"<h2 style='font-size: 20px; margin-top: 20px;'>üìã Column Names</h2>\"))\n    display(HTML(f\"<p>{', '.join(dataset.columns)}</p>\"))\n    \n    display(HTML(\"<h2 style='font-size: 20px; margin-top: 20px;'>üî¢ Unique Values</h2>\"))\n    unique_values_table = pd.DataFrame({\n        'Column Name': dataset.columns,\n        'Unique Values': [', '.join(map(str, dataset[col].unique()[:7])) + (', ...' if len(dataset[col].unique()) > 7 else '') for col in dataset.columns]\n    })\n    display(HTML(style_table(unique_values_table)))\n\n# Example usage with your dataset (`df_train`, `df_test`, `df_sub`)\ndf_train = pd.read_csv(\"/kaggle/input/playground-series-s4e7/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/playground-series-s4e7/test.csv\")\ndf_sub = pd.read_csv(\"/kaggle/input/playground-series-s4e7/sample_submission.csv\")\n\nprint_dataset_analysis(df_train, \"Training Data\", heading_color='#4B0082')  # Purple\nprint_dataset_analysis(df_test, \"Test Data\", heading_color='#1976d2')      # Blue\nprint_dataset_analysis(df_sub, \"Sample Solution\", heading_color='#388e3c') # Green","metadata":{"execution":{"iopub.status.busy":"2024-08-10T05:26:18.648657Z","iopub.execute_input":"2024-08-10T05:26:18.649106Z","iopub.status.idle":"2024-08-10T05:27:23.803128Z","shell.execute_reply.started":"2024-08-10T05:26:18.649078Z","shell.execute_reply":"2024-08-10T05:27:23.802387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Visualization\n\n\n* This code is used to perform the data visualization\n* It is used to create count plots for specific categorical features in the dataset which are being taken and it shows the distribution of each feature which is taken to create the plot.\n* Firstly create the figure by setting a specified size.\n* Then define Seaborn \"Set2\" palette so, that it provides a set of attractive colors for plotting.\n* Then create a list of categorical columns to be plotted.\n* Then it loops through each of the categorical variable which is taken and using the enumerate function which provide the index i and the feature name creates the suplots.\n* Then set the grid, xlabel, ylabel and the title.\n* Add the grid so that the plot formed with grid lines.\n* Then add boarder for each subplot and adjust the layout and shows the plot.","metadata":{}},{"cell_type":"markdown","source":"# Count Plot","metadata":{}},{"cell_type":"markdown","source":"* This code is used to perform the data visualization\n* It is used to create count plots for specific categorical features in the dataset which are being taken and it shows the distribution of each feature which is taken to create the plot.\n* Firstly create the figure by setting a specified size.\n* Then define Seaborn \"Viridis\" palette so, that it provides a set of attractive colors for plotting.\n* Then create a list of categorical columns to be plotted.\n* Then it loops through each of the categorical variable which is taken and using the enumerate function which provide the index i and the feature name creates the suplots.\n* Then set the grid, xlabel, ylabel and the title.\n* Add the grid so that the plot formed with grid lines.\n* Then add boarder for each subplot and adjust the layout and shows the plot.","metadata":{}},{"cell_type":"code","source":"# Load your data\ndf = pd.read_csv('/kaggle/input/playground-series-s4e7/train.csv')\n\n# Set up the matplotlib figure\nplt.figure(figsize=(15, 10))\n\n# Define a color palette with attractive colors\npalette = sns.color_palette(\"viridis\")\n\n# Create subplots for each categorical feature\ncategorical_features = ['Gender', 'Vehicle_Age', 'Vehicle_Damage']\n\nfor i, feature in enumerate(categorical_features, 1):\n    plt.subplot(2, 2, i)\n    \n    # Create the count plot\n    sns.countplot(data=df, x=feature, hue='Response', palette=palette, edgecolor='black')\n    \n    # Add background color and grid\n    plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n    plt.gca().set_facecolor('lightgrey')\n    \n    # Add titles and labels\n    plt.title(f'Count Plot of {feature}')\n    plt.xlabel(feature)\n    plt.ylabel('Count')\n    \n    # Add a container with a border around each subplot\n    box = plt.gca().patch\n    box.set_edgecolor('lightgrey')\n    box.set_linewidth(2)\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T05:27:23.804198Z","iopub.execute_input":"2024-08-10T05:27:23.804627Z","iopub.status.idle":"2024-08-10T05:28:48.330321Z","shell.execute_reply.started":"2024-08-10T05:27:23.804596Z","shell.execute_reply":"2024-08-10T05:28:48.329456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pair Plot","metadata":{}},{"cell_type":"markdown","source":"* This code is used to create the pairplot for the selected columns such as Region_Code, Annual_Premium, and Policy_Sales_Channel\n* Firstly the figure is set to create the plot. It help to craete the plot in such a way that the plots must be readable so to take clear insights about the data.\n* Then define Seaborn \"Viridis\" palette so, that it provides a set of attractive colors for plotting.It is one of the predefined color pelette of seaborn library.\n* Then by using sns.pairplot creates the pairplot of the columns such as 'Region_Code', 'Annual_Premium', 'Policy_Sales_Channel'\n* Then set the plot subtitle and set the size so that the plot is displaying clearly and we can take the deep insights about data.\n* Finally by using 'plt.show()' display the plot.\n* The paiplot is basically used to visualize the relationship among the numeric variables by forming.\n* The paiplot generates a matrix of plot where each variable is being plotted against another.\n* In the pairplot of these numeric columns the diagonal plot represented the kde plot for the numeric columns and the off diagonal plot shows the pairwise relationship between variable using scatterplot Hence, it is being used to determine the distribution among variables.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Load your data\ndf_train = pd.read_csv('/kaggle/input/playground-series-s4e7/train.csv')\n\n# Convert categorical columns to numeric if needed\ndf_train['Region_Code'] = df_train['Region_Code'].astype('category').cat.codes\ndf_train['Policy_Sales_Channel'] = df_train['Policy_Sales_Channel'].astype('category').cat.codes\n\n# Define the viridis colormap\nviridis_palette = sns.color_palette(\"viridis\", as_cmap=True)\n\n# Set up the matplotlib figure\nplt.figure(figsize=(15, 12))\n\n# Create a pair plot with the viridis colormap\npair_plot = sns.pairplot(df_train[['Region_Code', 'Annual_Premium', 'Policy_Sales_Channel']], \n                         diag_kind='kde', \n                         markers='o',\n                         palette=viridis_palette)\n\n# Show the plot with title\npair_plot.fig.suptitle('Pair Plot of Region_Code, Annual_Premium, and Policy_Sales_Channel', \n                       y=1.02)  # Adjust the title position\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-10T05:28:48.331470Z","iopub.execute_input":"2024-08-10T05:28:48.331756Z","iopub.status.idle":"2024-08-10T05:33:24.409228Z","shell.execute_reply.started":"2024-08-10T05:28:48.331727Z","shell.execute_reply":"2024-08-10T05:33:24.408519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Violin plot subplots","metadata":{}},{"cell_type":"markdown","source":"* This code is used to display the violin of numeric columns such as 'Age', 'Driving_License', 'Previously_Insured', 'Vintage'\n* A violin plot helps to determine the distributions of numeric columns using density curves. In this plot the width of each curve corresponds with the approximate frequency of data points in each region. * The wider the plot represents more density hence more datapoints presents there.\n* so, this plot help to determine the distribution and density of numeric variables.\n* Firstly the figure is set to create the plot. It help to craete the plot in such a way that the plots must be readable so to take clear insights about the data.\n* Then teh list of numeric columns such as 'Age', 'Driving_License', 'Previously_Insured', 'Vintage' is being defined.\n* Then for each plot colors are defined to get clear visualization\n* Then looping occurs using enumerate function so that it iterates over the numeric columns and the defined colors to create the clear plots.\n* Then creates the subplots grid and adjust the layout.\n* Finally displays the plot using plt.show()","metadata":{}},{"cell_type":"code","source":"# Set up the matplotlib figure\nplt.figure(figsize=(15, 12))\n\n# List of numerical features to plot\nnumerical_features = ['Age', 'Driving_License', 'Previously_Insured', 'Vintage']\ncolors = ['steelblue', 'darkorange', 'seagreen', 'crimson']\n\n# Create subplots for each numerical feature\nfor i, (feature, color) in enumerate(zip(numerical_features, colors), 1):\n    plt.subplot(2, 2, i)\n    \n    # Create the violin plot\n    sns.violinplot(data=df_train, x=feature, color=color, inner=\"quartile\", palette='viridis')\n    \n    # Add background color and grid\n    plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n    plt.gca().set_facecolor('lightgrey')\n    \n    # Add titles and labels\n    plt.title(f'Violin Plot of {feature}')\n    plt.xlabel(feature)\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T05:33:24.410215Z","iopub.execute_input":"2024-08-10T05:33:24.410480Z","iopub.status.idle":"2024-08-10T05:36:01.914002Z","shell.execute_reply.started":"2024-08-10T05:33:24.410455Z","shell.execute_reply":"2024-08-10T05:36:01.913089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Boxplot Subplots to Detect Outliers","metadata":{}},{"cell_type":"markdown","source":"* This plot is being used to create the boxplot of numeric features across different categories.\n* the boxplot is also used to detect the presence of outliers.\n* Firstly the figure is set to create the plot. It help to craete the plot in such a way that the plots must be readable so to take clear insights about the data.\n* Then the tuple of categorical and numerical columns is being defined so that the boxplot of them are being defined.\n* The numeric and categorical columns which is defined in the form of tuple as ('Age', 'Gender'),('Annual_Premium', 'Gender'),('Vintage', 'Gender'),('Region_Code', 'Gender'),('Age', 'Vehicle_Age'),('Annual_Premium', 'Vehicle_Age'),('Vintage', 'Vehicle_Age'),('Region_Code', 'Vehicle_Age'),('Age', 'Vehicle_Damage'),('Annual_Premium', 'Vehicle_Damage'),('Vintage', 'Vehicle_Damage'), ('Region_Code', 'Vehicle_Damage')\n* Then it loops through each of the categorical variable which is taken and using the enumerate function\n* Then creates the subplots grid and adjust the layout.\n* As this code generates the boxplot of the numeric and categorical columns so we can also detect the outliers by this plot such as the points which are outside of the whiskers are mainly counted as outliers","metadata":{}},{"cell_type":"code","source":"# Set up the matplotlib figure\nplt.figure(figsize=(18, 24))\n\n# Define the pairs of numeric and categorical columns\nplots = [\n    ('Age', 'Gender'),\n    ('Annual_Premium', 'Gender'),\n    ('Vintage', 'Gender'),\n    ('Region_Code', 'Gender'),\n    ('Age', 'Vehicle_Age'),\n    ('Annual_Premium', 'Vehicle_Age'),\n    ('Vintage', 'Vehicle_Age'),\n    ('Region_Code', 'Vehicle_Age'),\n    ('Age', 'Vehicle_Damage'),\n    ('Annual_Premium', 'Vehicle_Damage'),\n    ('Vintage', 'Vehicle_Damage'),\n    ('Region_Code', 'Vehicle_Damage')\n]\n\n# Create subplots for each pair\nfor i, (num_col, cat_col) in enumerate(plots, 1):\n    plt.subplot(6, 2, i)\n    \n    # Create the box plot\n    sns.boxplot(data=df_train, x=cat_col, y=num_col, palette='viridis')\n    \n    # Add background color and grid\n    plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n    plt.gca().set_facecolor('lightgrey')\n    \n    # Add titles and labels\n    plt.title(f'Box Plot of {num_col} by {cat_col}')\n    plt.xlabel(cat_col)\n    plt.ylabel(num_col)\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T05:36:01.915310Z","iopub.execute_input":"2024-08-10T05:36:01.915598Z","iopub.status.idle":"2024-08-10T05:41:07.140090Z","shell.execute_reply.started":"2024-08-10T05:36:01.915572Z","shell.execute_reply":"2024-08-10T05:41:07.139170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Winsorized Data with Box Plot Subplots","metadata":{}},{"cell_type":"markdown","source":"* This code is used to create the box plot\n* Firstly the winsorization technique is being applied using this code before plotting the boxplot of the numeric variables.\n* winsorization involves the transformation of statistics by limiting extreme values in the statistical data to reduce the effect of possibly spurious outliers.\n* It is the process of replacing the extreme values of statistical data in order to limit the effect of the outliers on the calculations.\n* The mean value calculated after such replacement of the extreme values is called winsorized mean.\n* winsorization process involves replacing the outlier values with the nearest non-outlier values.\n* The winsorization method is used to limit the outliers without removing the rows of the data.\n* winsorize_column(column, limits=(0.01, 0.01)): This function is used to apply winsorization to the columns of the data so that it limits the outliers without removing rows.\n* Loop through each column using the enumerate function.\n* Then the figure is set to craete the plot. It help to craete the plot in such a way that the plots must be readable so to take clear insights about the data.\n* Then the boxplot formed of numeric and categorical columns which are taken in the form of tuple.\n* Then creates the subplots grid and adjust the layout.\n* Finally plot is being displayed using plt.show()","metadata":{}},{"cell_type":"code","source":"# Function to apply Winsorization to numeric columns\ndef winsorize_column(column, limits=(0.01, 0.01)):\n    return winsorize(column, limits=limits)\n\n# Apply Winsorization to numeric columns\nnumeric_cols = ['Age', 'Annual_Premium', 'Vintage', 'Region_Code']\nfor col in numeric_cols:\n    df_train[col] = winsorize_column(df_train[col])\n\n# Set up the matplotlib figure\nplt.figure(figsize=(18, 24))\n\n# Define the pairs of numeric and categorical columns\nplots = [\n    ('Age', 'Gender'),\n    ('Annual_Premium', 'Gender'),\n    ('Vintage', 'Gender'),\n    ('Region_Code', 'Gender'),\n    ('Age', 'Vehicle_Age'),\n    ('Annual_Premium', 'Vehicle_Age'),\n    ('Vintage', 'Vehicle_Age'),\n    ('Region_Code', 'Vehicle_Age'),\n    ('Age', 'Vehicle_Damage'),\n    ('Annual_Premium', 'Vehicle_Damage'),\n    ('Vintage', 'Vehicle_Damage'),\n    ('Region_Code', 'Vehicle_Damage')\n]\n\n# Create subplots for each pair\nfor i, (num_col, cat_col) in enumerate(plots, 1):\n    plt.subplot(6, 2, i)\n    \n    # Create the box plot\n    sns.boxplot(data=df_train, x=cat_col, y=num_col, palette='viridis')\n    \n    # Add background color and grid\n    plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n    plt.gca().set_facecolor('lightgrey')\n    \n    # Add titles and labels\n    plt.title(f'Box Plot of {num_col} by {cat_col}')\n    plt.xlabel(cat_col)\n    plt.ylabel(num_col)\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T05:41:07.141361Z","iopub.execute_input":"2024-08-10T05:41:07.141648Z","iopub.status.idle":"2024-08-10T05:46:13.696666Z","shell.execute_reply.started":"2024-08-10T05:41:07.141622Z","shell.execute_reply":"2024-08-10T05:46:13.695726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pie Chart\n\n\n* This code is used to create the pie chart of the categorical columns such as 'Gender', 'Vehicle_Age', 'Vehicle_Damage', 'Response'.\n* Firstly define Seaborn \"viridis\" palette so, that it provides a set of attractive colors for plotting.It is one of the predefined color pelette of seaborn library.\n* Then the figure is set to create the plot. It help to craete the plot in such a way that the plots must be readable so to take clear insights about the data.\n* Then loop through each defined categorical columns such as 'Gender', 'Vehicle_Age', 'Vehicle_Damage', 'Response' using the enumeate function.\n* Then creates the grid of subplot and set the title and ajust the layout for creating subplot\n* Finally Displays the plot using plt.show()","metadata":{}},{"cell_type":"code","source":"# Set up the matplotlib figure\nplt.figure(figsize=(10, 10))\n\n# Define a color palette with attractive colors\npalette = sns.color_palette(\"crest\")\n\n# Create subplots for each categorical feature\ncategorical_features = ['Gender', 'Vehicle_Age', 'Vehicle_Damage', 'Response']\n\nfor i, feature in enumerate(categorical_features, 1):\n    plt.subplot(2, 2, i)\n    \n    # Get the count of each category\n    counts = df_train[feature].value_counts()\n    \n    # Create the pie chart\n    plt.pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=140, colors=palette)\n    \n    # Add titles\n    plt.title(f'Pie Chart of {feature}')\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-10T05:46:13.699157Z","iopub.execute_input":"2024-08-10T05:46:13.699478Z","iopub.status.idle":"2024-08-10T05:46:16.226792Z","shell.execute_reply.started":"2024-08-10T05:46:13.699451Z","shell.execute_reply":"2024-08-10T05:46:16.225870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"* Import Necessary Libraries: Ensure you have all the necessary imports at the top of your script.\n* Improved Frequency Encoding: Frequency encoding is correctly implemented. Ensure you handle the case when a Region_Code might not be present in the training set but appears in the test set.\n* Handling Missing Values: Add handling for any potential missing values, especially after transformations.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\n\ndef feature_engineering(df):\n    # Ensure no missing values in categorical features before mapping\n    df['Vehicle_Age'].fillna('Unknown', inplace=True)\n    df['Vehicle_Damage'].fillna('Unknown', inplace=True)\n    df['Gender'].fillna('Unknown', inplace=True)\n    \n    # Transforming categorical variables\n    df['Vehicle_Age'] = df['Vehicle_Age'].map({'< 1 Year': 0, '1-2 Year': 1, '> 2 Years': 2})\n    df['Vehicle_Damage'] = df['Vehicle_Damage'].map({'Yes': 1, 'No': 0})\n    df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n\n    # Ensure no missing values in numerical features after transformation\n    df['Vehicle_Age'].fillna(df['Vehicle_Age'].median(), inplace=True)\n    df['Vehicle_Damage'].fillna(df['Vehicle_Damage'].median(), inplace=True)\n    df['Gender'].fillna(df['Gender'].median(), inplace=True)\n    \n    # Create interaction features\n    df['Age*Annual_Premium'] = df['Age'] * df['Annual_Premium']\n    df['Region_Code*Policy_Sales_Channel'] = df['Region_Code'] * df['Policy_Sales_Channel']\n    df['Age*Vehicle_Damage'] = df['Age'] * df['Vehicle_Damage']\n    df['Annual_Premium*Vehicle_Age'] = df['Annual_Premium'] * df['Vehicle_Age']\n    \n    # Binning Annual_Premium\n    bins = [0, 10000, 30000, 50000, np.inf]\n    labels = [0, 1, 2, 3]\n    df['Annual_Premium_binned'] = pd.cut(df['Annual_Premium'], bins=bins, labels=labels)\n    \n    # Frequency encoding for Region_Code\n    freq = df['Region_Code'].value_counts(normalize=True)\n    df['Region_Code_freq'] = df['Region_Code'].map(freq)\n    \n    # Label encoding for categorical features\n    cat_features = ['Annual_Premium_binned']\n    le = LabelEncoder()\n    for feature in cat_features:\n        df[feature] = le.fit_transform(df[feature].astype(str))\n    \n    return df\n","metadata":{"execution":{"iopub.status.busy":"2024-08-10T05:46:16.228027Z","iopub.execute_input":"2024-08-10T05:46:16.228322Z","iopub.status.idle":"2024-08-10T05:46:16.237791Z","shell.execute_reply.started":"2024-08-10T05:46:16.228296Z","shell.execute_reply":"2024-08-10T05:46:16.237002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Efficient Model Training and Stacking for Enhanced Predictions","metadata":{}},{"cell_type":"markdown","source":"* Imports and Data Preparation:\n* Ensure all necessary imports are included.\n* Verify that df_train and df_test are defined correctly and that df_test has the same preprocessing applied as df_train.\n* Model Training and Validation:\n* Cross-validation is set up correctly for evaluating the LightGBM model.\n* The roc_auc metric is used, which is appropriate for binary classification.\n* Stacking Model:\n* A stacking model is defined, but make sure all estimators are properly trained and validated.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import StackingClassifier\n\n# Ensure you have already defined the feature_engineering function\ndef feature_engineering(df):\n    # Transforming categorical variables\n    df['Vehicle_Age'] = df['Vehicle_Age'].map({'< 1 Year': 0, '1-2 Year': 1, '> 2 Years': 2})\n    df['Vehicle_Damage'] = df['Vehicle_Damage'].map({'Yes': 1, 'No': 0})\n    df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n\n    # Create interaction features\n    df['Age*Annual_Premium'] = df['Age'] * df['Annual_Premium']\n    df['Region_Code*Policy_Sales_Channel'] = df['Region_Code'] * df['Policy_Sales_Channel']\n    df['Age*Vehicle_Damage'] = df['Age'] * df['Vehicle_Damage']\n    df['Annual_Premium*Vehicle_Age'] = df['Annual_Premium'] * df['Vehicle_Age']\n    \n    # Binning Annual_Premium\n    bins = [0, 10000, 30000, 50000, np.inf]\n    labels = [0, 1, 2, 3]\n    df['Annual_Premium_binned'] = pd.cut(df['Annual_Premium'], bins=bins, labels=labels)\n    \n    # Frequency encoding for Region_Code\n    freq = df['Region_Code'].value_counts(normalize=True)\n    df['Region_Code_freq'] = df['Region_Code'].map(freq)\n    \n    # Label encoding for categorical features\n    cat_features = ['Annual_Premium_binned']\n    le = LabelEncoder()\n    for feature in cat_features:\n        df[feature] = le.fit_transform(df[feature].astype(str))\n    \n    return df\n\n# Apply feature engineering\ndf_train = pd.read_csv('/kaggle/input/playground-series-s4e7/train.csv')\ndf_test = pd.read_csv('/kaggle/input/playground-series-s4e7/test.csv')\ntrain_data_fe = feature_engineering(df_train)\ntest_data_fe = feature_engineering(df_test)\n\n# Prepare the data for modeling\nX = train_data_fe.drop(['id', 'Response'], axis=1)\ny = train_data_fe['Response']\n\n# Train-test split for validation\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n\n# Define LightGBM parameters and model\nparams = {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'random_state': 42,\n    'learning_rate': 0.05,\n    'num_leaves': 40,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.9,\n    'lambda_l1': 0.2,\n    'lambda_l2': 0.2,\n    'min_child_samples': 20,\n    'n_estimators': 1000\n}\n\n# LightGBM model\nlgb_model = lgb.LGBMClassifier(**params)\n\n# Train the LightGBM model using cross-validation\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ncv_scores = cross_val_score(lgb_model, X_train, y_train, cv=kfold, scoring='roc_auc', verbose=1)\n\nprint(f'Cross-validation ROC AUC scores: {cv_scores}')\nprint(f'Mean ROC AUC score: {cv_scores.mean()}')\n\n# Fit the model on the entire training data\nlgb_model.fit(X_train, y_train)\n\n# Make predictions on the validation data\nval_predictions = lgb_model.predict_proba(X_val)[:, 1]\nval_auc = roc_auc_score(y_val, val_predictions)\nprint(f'Validation ROC AUC score: {val_auc}')\n\n# Create a stacking model\nestimators = [\n    ('lgbm', lgb_model),\n    ('log_reg', LogisticRegression(random_state=42))\n]\n\nstacking_model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), cv=kfold)\n\n# Train the stacking model\nstacking_model.fit(X_train, y_train)\n\n# Make predictions with the stacking model (optional, for further evaluation or submission)\nstacking_predictions = stacking_model.predict_proba(X_val)[:, 1]\nstacking_auc = roc_auc_score(y_val, stacking_predictions)\nprint(f'Stacking Model Validation ROC AUC score: {stacking_auc}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-10T05:46:16.238791Z","iopub.execute_input":"2024-08-10T05:46:16.239040Z","iopub.status.idle":"2024-08-10T06:05:55.961944Z","shell.execute_reply.started":"2024-08-10T05:46:16.239017Z","shell.execute_reply":"2024-08-10T06:05:55.960746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make predictions on the test data with stacking Model","metadata":{}},{"cell_type":"code","source":"\n# Make predictions on the validation data with stacking model\nstacking_val_predictions = stacking_model.predict_proba(X_val)[:, 1]\nstacking_val_auc = roc_auc_score(y_val, stacking_val_predictions)\nprint(f'Stacking model validation ROC AUC score: {stacking_val_auc}')\n\n# Make predictions on the test data with stacking model\ntest_predictions = stacking_model.predict_proba(test_data_fe.drop(['id'], axis=1))[:, 1]","metadata":{"execution":{"iopub.status.busy":"2024-08-10T06:05:55.963203Z","iopub.execute_input":"2024-08-10T06:05:55.963532Z","iopub.status.idle":"2024-08-10T06:06:28.557439Z","shell.execute_reply.started":"2024-08-10T06:05:55.963503Z","shell.execute_reply":"2024-08-10T06:06:28.556420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Cross-validation ROC AUC scores: {cv_scores}')\nprint(f'Mean ROC AUC score: {cv_scores.mean()}')\nprint(f'Stacking model validation ROC AUC score: {stacking_val_auc}')","metadata":{"execution":{"iopub.status.busy":"2024-08-10T06:06:28.558617Z","iopub.execute_input":"2024-08-10T06:06:28.558908Z","iopub.status.idle":"2024-08-10T06:06:28.564053Z","shell.execute_reply.started":"2024-08-10T06:06:28.558880Z","shell.execute_reply":"2024-08-10T06:06:28.563300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create the Submission File","metadata":{}},{"cell_type":"code","source":"# Create submission file\nsubmission = pd.DataFrame({\n    'id': test_data_fe['id'],\n    'Response': test_predictions\n})\n\n# Save submission to CSV\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file created successfully.\")\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T06:06:28.564977Z","iopub.execute_input":"2024-08-10T06:06:28.565216Z","iopub.status.idle":"2024-08-10T06:06:44.143290Z","shell.execute_reply.started":"2024-08-10T06:06:28.565193Z","shell.execute_reply":"2024-08-10T06:06:44.142472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Binary Classification of Insurance Cross-Selling Competition Data is well-structured and covers key steps from data exploration to model evaluation and submission. Here‚Äôs a more detailed breakdown of each step and what you should consider:\n\n1. Detailed Data Overview\nObjective: Understand the structure, types, and distribution of your data.\nSteps:\n* Check for missing values and basic statistics.\n* Identify the types of features (categorical, numerical).\n* Examine class distribution in the target variable.\n2. Data Visualization\n* Countplot: To visualize the distribution of categorical features and how they relate to the target variable.\n* Pairplot: To explore relationships between numerical features and check for potential correlations.\n* Violin Plot: To compare distributions of numerical features across different categories.\n* Pie Chart: To visualize the proportion of classes in the target variable.","metadata":{}},{"cell_type":"markdown","source":"3. Outlier Detection and Winsorization\n* Boxplot: To visualize the presence of outliers in numerical features.\n* Winsorization: To handle outliers by capping extreme values to reduce their impact on model performance.","metadata":{}},{"cell_type":"markdown","source":"4. Enhanced Feature Engineering\n* Transform categorical variables: Convert to numeric if needed.\n* Interaction Features: Create new features that capture interactions between existing features.\n* Binning: For features like Annual_Premium.\n* Frequency Encoding: For categorical features.","metadata":{}},{"cell_type":"markdown","source":"5. Model Training and Stacking\n* Model Training: Train base models using algorithms like LightGBM and validate their performance using cross-validation.\n* Stacking: Combine multiple models to leverage their individual strengths for improved performance.","metadata":{}},{"cell_type":"markdown","source":"6. Create the Submission File\n* Generate Predictions: Use the final model to make predictions on the test set.\n* Format the Submission: Prepare the output in the required format for submission.","metadata":{}},{"cell_type":"markdown","source":"Summary\n* Data Exploration and Visualization: Gain insights and understand data distributions.\n* Outlier Detection and Feature Engineering: Enhance model performance with better features.\n* Model Training and Validation: Use LightGBM and stacking to improve predictions.\n* Submission: Prepare and save predictions for the competition.","metadata":{}}]}